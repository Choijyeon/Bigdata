{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] SparkSession 설정\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.master('local').appName('mllib-first-exercise').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib 예제: Logistic Regression\n",
    "+ pyspark.ml.linalg: 선형대수(linear algebra) 관련 패키지\n",
    "    + Vectors: 벡터 데이터 타입\n",
    "+ pyspark.ml.classification: 분류 모델 관련 패키지\n",
    "    + LogisticRegression: 가장 기본적인 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors, LogisticRegression 임포트\n",
    "from pyspark.ml.linalg import Vectors # ml이라는 패키지 안에 들어있는 것 -> 선형대수\n",
    "from pyspark.ml.classification import LogisticRegression # ml이라는 패키지 안에 들어있는 것 -> 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    데이터 정의: 레이블(정답)과 특징\n",
    "    - dense(): 모든 값을 저장하는 벡터 생성\n",
    "    - sparse(): 0이 아닌 값만 저장하는 벡터 생성\n",
    "\"\"\"\n",
    "\n",
    "train_data = [\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5])),\n",
    "]\n",
    "# 첫 번째 컬럼 : 레이블, 뒤의 연속된 값들은 특징들 / 1인지 0인지만 분류하는 것. 데이터가 무엇인지는 의미 X\n",
    "# dense : 촘촘하다는 개념(반대로는 스파스? 희소. 예를 들어 column은 백만개인데 값은 10개)\n",
    "# 희소 행렬은 그냥 index를 사용하는 것이 나음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] 스키마 정의\n",
    "schema = ['label', 'features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] DataFrame 생성\n",
    "train_df = ss.createDataFrame(train_data, schema=schema) # 훈련데이터에 대한 데이터 프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|  1.0| [0.0,1.1,0.1]|\n",
      "|  0.0|[2.0,1.0,-1.0]|\n",
      "|  0.0| [2.0,1.3,1.0]|\n",
      "|  1.0|[0.0,1.2,-0.5]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    [+] LogisticRegression (Estimator) 생성\n",
    "    - maxIter: 최대 학습 횟수 매개변수\n",
    "    - regParam: 정규화 매개변수\n",
    "\"\"\"\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001) # maxiter는 최대횟수 / regParam은 매개변수..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] 모델 학습 -> 보라색 선(두 클래스를 분할해주는 선)을 만들어주는 것\n",
    "model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_c5187d0cee6d, numClasses=2, numFeatures=3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 테스트\n",
    "test_data = [\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5])),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] DataFrame 생성\n",
    "test_df = ss.createDataFrame(test_data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] 모델 예측\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0|[-1.0,1.5,1.3]|[-10.024339064172...|[4.43063136183005...|       1.0|\n",
      "|  0.0|[3.0,2.0,-0.1]|[8.88090306449148...|[0.99986100075734...|       0.0|\n",
      "|  1.0|[0.0,2.2,-1.5]|[-6.7315196407454...|[0.00119129819168...|       1.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측결과 출력\n",
    "predictions.show()\n",
    "# rawPrediction은 중간 결과 보여주는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark 이용해서 머신러닝 과정 한 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
